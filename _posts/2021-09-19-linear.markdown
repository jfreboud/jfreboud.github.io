---
layout: post
title:  "The Linear Layer"
date:   2021-09-19
excerpt: >-
  We explore the linear layer. It is the first step to be able to design deep learning models. 
  We also speak about the neural structure and a better way to compute the backward pass.
---

## Introduction

It is time to explore the real form of the different possible layers of our deep learning $ model $.
As we already know, the $ model $ is a graph of ordered $ layers $ 
(see [this article]({% post_url 2021-08-06-inside-the-model %})). Each of the different $ layers $ will 
progressively build a richer and more complex understanding of the **data input** of our **dataset**. 

Each $ layer $ will in fact add its own characteristics to the previous understanding built by its previous 
$ layers $ in the **forward pass** order. Said differently we can see this graph of $ layers $ as a building site. 
The first $ layers $ will build the foundations. The last $ layers $ will add the finishes. 

In the coming articles, we will speak about the characteristics of the different types of $ layer $. 
We will concentrate on the $ layers $ that actually **learn** something: $ layers $ declaring **weights** 
(recall the **weights** in [this article]({% post_url 2021-08-19-weights %})). 

Let us begin with the $ Linear $ $ layer $.

## The Linear neural structure

In order to better understand the function realized by one $ Linear $ $ layer $, we will explore its 
**neural structure**. 

Let us take a $ L^{k+1} $ $ Linear $ $ layer $. As every $ Linear $ $ layer $ it is a **learning** $ layer $, 
which means it declares **weights** (see the [weights article]({% post_url 2021-08-19-weights %})). 
Let us note $ W^{k+1} $ these **weights**.

By definition of the **forward pass** (see [this article]({% post_url 2021-08-06-inside-the-model %})) 
we need $ L^{k+1} $'s previous layer $ L^{k} $ in order to evaluate $ L^{k+1} $ on the outputs of $ L^{k} $. 
For now this is how we represent this relation:

![Linear](/_assets/images/layers/Linear1.png)

In the diagram, we are now ready to remove the clouds and turn them into **neurons**! 
Each **neuron** represents the $ X $ variable (input of one $ layer $), the $ L(X) $ variable (output of one $ layer $) 
or their value evaluation during the **learning phase** or the **inferring phase**.

Let us suppose $ L^{k} $ is a $ Linear $ $ layer $ with 3 **neurons**. This literally means 
$ L^{k} $ produces 3 outputs: $ o^{k}_1 $, $ o^{k}_2 $ and $ o^{k}_3 $. 

Let us suppose $ L^{k+1} $ produces 2 output **neurons**: $ o^{k+1}_1 $ and $ o^{k+1}_2 $. 

The diagram becomes: 

![Linear](/_assets/images/layers/Linear2.png)

Note that the number of **neurons** for each $ layer $ is up to the developer. The fact that we chose 2 
output **neurons** for $ L^{k+1} $ and 3 output **neurons** for $ L^{k} $ is just an example. We could also have 
decided that $ L^{k+1} $ produces 20 945 output **neurons** but it would have been less practical to draw a diagram. 

The final question is: what is the form of the $ L^{k+1} $ function ? 
We just know it transforms the outputs of $ L^{k} $ into the very own outputs of $ L^{k+1} $.

The answer is: we do not know :smiling_imp: 

This is the reason why we do some crazy move: connecting each $ L^{k} $ **neuron** 
to each $ L^{k+1} $ **neuron**. The connexions being assured by the **weights** of $ L^{k+1} $: $ W^{k+1} $.

But there is one subtlety here: $ L^{k+1} $ has two **neurons**. So each $ L^{k+1} $ **neuron** will be connected 
to every **neurons** of $ L^{k} $. This is the main characteristic of the $ Linear $ $ layer $: connecting 
every output **neurons** to every input **neurons**.

Concretely, we must declare $ W^{k+1, 1} $ **weights** that will connect the different input **neurons**: 
$ o^{k}_1 $, $ o^{k}_2 $ and $ o^{k}_3 $ to the first output **neuron** $ o^{k+1}_1 $.

<a id="linear-structure1" class="anchor">
![Linear](/_assets/images/layers/Linear3.png)
</a>

And we must also declare $ W^{k+1, 2} $ **weights** that will connect the different input **neurons**: 
$ o^{k}_1 $, $ o^{k}_2 $ and $ o^{k}_3 $ to the second output **neuron** $ o^{k+1}_2 $.

<a id="linear-structure2" class="anchor">
![Linear](/_assets/images/layers/Linear4.png)
</a>

## Forward pass

Using the same $ L^{k+1} $ $ layer $ as in the [previous paragraph](#the-linear-neural-structure), we have:

$$ 
\begin{align}
L^{k+1, 1}(X^{k+1}) &= W^{k+1, 1}_1 . X^{k+1, 1} + W^{k+1, 1}_2 . X^{k+1, 2} + W^{k+1, 1}_3 . X^{k+1, 3} + B^{k+1} \\
L^{k+1, 2}(X^{k+1}) &= W^{k+1, 2}_1 . X^{k+1, 1} + W^{k+1, 2}_2 . X^{k+1, 2} + W^{k+1, 2}_3 . X^{k+1, 3} + B^{k+1}
\end{align}
$$ 

- $ X^{k+1} $ is the natural dependency of $ L^{k+1} $. It receives the outputs of the previous $ L^{k} $ $ layer $. 

- $ W^{k+1} $ are the **weights** declared by the $ L^{k+1} $ $ layer $. These **weights** will receive their values 
from the developer, beginning with a start value. Then they are **updated** during the **learning phase** 
(see the [weights article]({% post_url 2021-08-19-weights %})).

- $ B^{k+1} $ is another **weight** variable, called the **bias**. 
Their initial value is typically 0. They are **updated** during the **learning phase** too.

Let us check the structure of the $ L^{k+1} $ $ layer $. We have used the same structure as in the 
[previous paragraph](#the-linear-neural-structure): we should be able to identify 2 outputs. Each of these 2 outputs 
depending of the 3 outputs of the previous $ L^{k} $ $ layer $.

Indeed, we have $ L^{k+1, 1} $ and $ L^{k+1, 2} $ which are the two output **neurons**.
Each of these variables is connected to $ X^{k+1, 1} $, $ X^{k+1, 2} $ and $ X^{k+1, 3} $ which are the 3 input 
**neurons**, or said differently the 3 output **neurons** of the previous $ layer $. 

What is interesting to note is that we have declared 2 (output **neurons**) * 3 (input **neurons**) = 6 weights 
in total: 
$ W^{k+1, 1}_1 $, $ W^{k+1, 1}_2 $, $ W^{k+1, 1}_3 $, $ W^{k+1, 2}_1 $, $ W^{k+1, 2}_2 $ and $ W^{k+1, 2}_3 $.

Note that we have one more **weight**: $ B^{k+1} $ that is used the same in $ L^{k+1, 1} $ and in $ L^{k+1, 2} $. 
This will be important in the [backward pass paragraph](#backward-pass-for-the-weights).

## Backward pass

As we saw in the [weights article]({% post_url 2021-08-19-weights %}), the goal of the **backward pass** is to compute 

$$ 
\boxed{\delta w = \frac{\partial Loss}{\partial W}(x, y^{truth})}
$$

for each **weight** of every $ layer $. This is the direction to follow in the **weights**' **update** in order 
to minimize the $ Loss $ function.

We also have to use the **backward pass** in order to back propagate

$$
\boxed{\delta = \frac{\partial Loss}{\partial X}(x, y^{truth})}
$$

which is the **learning flow**: the essential part to compute the direction in the first formula. Let us begin with 
this **learning flow**. 

## Backward pass for the learning flow 

We are currently focusing on the $ L^{k+1} $ $ layer $, trying to compute:

$$ 
\delta^{k+1} = \frac{\partial Loss}{\partial X^{k+1}}(o^k)
$$

In the [backward pass article]({% post_url 2021-08-13-backward-pass %}), we would use the **chain rule** in order 
to compute the explicit formula for $ \frac{\partial Loss}{\partial X^{k+1}} $.

We will see how to obtain this $ \delta^{k+1} $ with a more straight forward approach. 

The principal idea is to go back to the very structure of $ L^{k+1} $ in order to find the impacts of $ X^{k+1} $ 
on the $ Loss $ function, knowing that the "future" 
**learning flow** has already been computed (by definition of the **backward pass**). 

The structure for the $ L^{k+1} $ $ layer $ is: 
- 2 output **neurons** 
- 3 input **neurons**. 

$ \delta^{k+2, 1} $ and $ \delta^{k+2, 2} $ are the "future" **learning flow**: we admit they have already been 
computed.
We must back propagate the **learning flow** to $ \delta^{k+1, 1} $, $ \delta^{k+1, 2} $ and $ \delta^{k+1, 3} $.

![Linear](/_assets/images/layers/Linear5.png)

### Computing $ \delta^{k+1, 1} $ 

$$ 
\delta^{k+1, 1} = \frac{\partial Loss}{\partial X^{k+1, 1}}(o^k_1)
$$

The interesting variable is $ X^{k+1, 1} $. In the different diagrams it corresponds to $ o^{k}_1 $, 
its value during the current **backward pass**. Let us find its impacts on the $ Loss $ function.
In fact, every output of $ L^{k+1} $ uses $ X^{k+1, 1} $.
Indeed we can look at the two diagrams [here](#linear-structure1) to see that 
$ o^{k+1}_1 $ is linked to $ o^{k}_1 $ and that $ o^{k+1}_2 $ is also linked to $ o^{k}_1 $.

We have already seen examples ([backward pass]({% post_url 2021-08-13-backward-pass %}) and 
[batch learning](% post_url 2021-08-24-batch-learning %)) with multiple impacts: in the end we just added them 
all together.

We are now able to build the **paths** of impacts from $ X^{k+1, 1} $ to the $ Loss $ function. 

![Linear](/_assets/images/layers/Linear6.png)

- $ X^{k+1, 1} $ impacts $ L^{k+1, 1} $ (definition of $ L^{k+1} $) which impacts the $ Loss $ function ("future" **learning flow**)
- $ X^{k+1, 1} $ impacts $ L^{k+1, 2} $ (definition of $ L^{k+1} $) which impacts the $ Loss $ function ("future" **learning flow**)

We add these 2 impacts, using the **chain rule**, to obtain: 

$$ 
\delta^{k+1, 1} = \delta^{k+2, 1} * \frac{\partial L^{k+1, 1}}{X^{k+1, 1}}(o^k_1)  + 
\delta^{k+2, 2} * \frac{\partial L^{k+1, 2}}{X^{k+1, 1}}(o^k_1)
$$

We just have to compute: 

$$ 
\begin{align}
\frac{\partial L^{k+1, 1}}{\partial X^{k+1, 1}} &= 
\frac{\partial (W^{k+1, 1}_1 . X^{k+1, 1} + W^{k+1, 1}_2 . X^{k+1, 2} + W^{k+1, 1}_3 . X^{k+1, 3} + B^{k+1})}{\partial X^{k+1, 1}} \\
                                                &= W^{k+1, 1}_1 
\end{align}
$$

Then we evaluate this function on the values that have produced the final $ loss $:

$$ 
\frac{\partial L^{k+1, 1}}{X^{k+1, 1}}(o^k_1) = w^{k+1, 1}_1 
$$

We do the same to obtain: 

$$ 
\frac{\partial L^{k+1, 2}}{X^{k+1, 1}}(o^k_1) = w^{k+1, 2}_1
$$

We finally assemble these two results:

$$ 
\boxed{\delta^{k+1, 1} = \delta^{k+2, 1} * w^{k+1, 1}_1 + \delta^{k+2, 2} * w^{k+1, 2}_1}
$$

### Computing $ \delta^{k+1, 2} $ 

$$ 
\delta^{k+1, 2} = \frac{\partial Loss}{\partial X^{k+1, 2}}(o^k_2)
$$

The interesting variable is $ X^{k+1, 2} $. In the different diagrams it corresponds to $ o^{k}_2 $, 
its value during the current **backward pass**. As in the previous paragraph, 
every output of $ L^{k+1} $ uses $ X^{k+1, 2} $.
Indeed we can look at the last two diagrams [here](#linear-structure1) to see that 
$ o^{k+1}_1 $ is linked to $ o^{k}_2 $ and that $ o^{k+1}_2 $ is also linked to $ o^{k}_2 $.

We are now able to build the **paths** of impacts from $ X^{k+1, 2} $ to the $ Loss $ function. 

![Linear](/_assets/images/layers/Linear7.png)

- $ X^{k+1, 2} $ impacts $ L^{k+1, 1} $ which impacts the $ Loss $ function 
- $ X^{k+1, 2} $ impacts $ L^{k+1, 2} $ which impacts the $ Loss $ function 

We add these 2 impacts, using the **chain rule**, to obtain: 

$$ 
\delta^{k+1, 2} = \delta^{k+2, 1} * \frac{\partial L^{k+1, 1}}{X^{k+1, 2}}(o^k_2)  + 
\delta^{k+2, 2} * \frac{\partial L^{k+1, 2}}{X^{k+1, 2}}(o^k_2)
$$

We just have to compute: 

$$ 
\begin{align}
\frac{\partial L^{k+1, 1}}{\partial X^{k+1, 2}} &= 
\frac{\partial (W^{k+1, 1}_1 . X^{k+1, 1} + W^{k+1, 1}_2 . X^{k+1, 2} + W^{k+1, 1}_3 . X^{k+1, 3} + B^{k+1})}{\partial X^{k+1, 2}} \\
                                                &= W^{k+1, 1}_2 
\end{align}
$$

Then we evaluate this function on the values that have produced the final $ loss $:

$$ 
\frac{\partial L^{k+1, 1}}{X^{k+1, 2}}(o^k_2) = w^{k+1, 1}_2 
$$

We do the same to obtain: 

$$ 
\frac{\partial L^{k+1, 2}}{X^{k+1, 2}}(o^k_2) = w^{k+1, 2}_2
$$

We finally assemble these two results:

$$ 
\boxed{\delta^{k+1, 2} = \delta^{k+2, 1} * w^{k+1, 1}_2 + \delta^{k+2, 2} * w^{k+1, 2}_2}
$$

### Computing $ \delta^{k+1, 3} $ 

$$ 
\delta^{k+1, 3} = \frac{\partial Loss}{\partial X^{k+1, 3}}(o^k_3)
$$

The interesting variable is $ X^{k+1, 3} $. In the different diagrams it corresponds to $ o^{k}_3 $, 
its value during the current **backward pass**. As in the previous paragraph, 
every output of $ L^{k+1} $ uses $ X^{k+1, 3} $.
Indeed we can look at the last two diagrams [here](#linear-structure1) to see that 
$ o^{k+1}_1 $ is linked to $ o^{k}_3 $ and that $ o^{k+1}_2 $ is also linked to $ o^{k}_3 $.

We are now able to build the **paths** of impacts from $ X^{k+1, 3} $ to the $ Loss $ function. 

![Linear](/_assets/images/layers/Linear8.png)

- $ X^{k+1, 3} $ impacts $ L^{k+1, 1} $ which impacts the $ Loss $ function 
- $ X^{k+1, 3} $ impacts $ L^{k+1, 2} $ which impacts the $ Loss $ function 

We add these 2 impacts, using the **chain rule**, to obtain: 

$$ 
\delta^{k+1, 3} = \delta^{k+2, 1} * \frac{\partial L^{k+1, 1}}{X^{k+1, 3}}(o^k_3)  + 
\delta^{k+2, 2} * \frac{\partial L^{k+1, 2}}{X^{k+1, 3}}(o^k_3)
$$

We just have to compute: 

$$ 
\begin{align}
\frac{\partial L^{k+1, 1}}{\partial X^{k+1, 3}} &= 
\frac{\partial (W^{k+1, 1}_1 . X^{k+1, 1} + W^{k+1, 1}_2 . X^{k+1, 2} + W^{k+1, 1}_3 . X^{k+1, 3} + B^{k+1})}{\partial X^{k+1, 3}} \\
                                                &= W^{k+1, 1}_3 
\end{align}
$$

Then we evaluate this function on the values that have produced the final $ loss $:

$$ 
\frac{\partial L^{k+1, 1}}{X^{k+1, 3}}(o^k_3) = w^{k+1, 1}_3 
$$

We do the same to obtain: 

$$ 
\frac{\partial L^{k+1, 2}}{X^{k+1, 3}}(o^k_3) = w^{k+1, 2}_3
$$

We finally assemble these two results:

$$ 
\boxed{\delta^{k+1, 3} = \delta^{k+2, 1} * w^{k+1, 1}_3 + \delta^{k+2, 2} * w^{k+1, 2}_3}
$$

## Backward pass for the weights

In this paragraph we continue focusing on the $ L^{k+1} $ $ layer $. This time we want to compute 
the direction of its **weights**' **update**: 

$$ 
\delta w^{k+1} = \frac{\partial Loss}{\partial W}(o^k)
$$

We will use the exact same strategy as in the [last paragraph](#backward-pass-for-the-learning-flow). 
The principal idea is to go back to the very structure of $ L^{k+1} $ in order to find the impacts of $ W^{k+1} $ 
on the $ Loss $ function, knowing that the "future" 
**learning flow** has already been computed (by definition of the **backward pass**). 

<a id="linear-structure3" class="anchor">
![Linear](/_assets/images/layers/Linear9.png)
</a>

<a id="linear-structure4" class="anchor">
![Linear](/_assets/images/layers/Linear10.png)
</a>

There is one last **weight** we have to **update** during the **learning phase**: $ B^{k+1} $, the **bias**. 
Thus, we will have to compute $ \delta b^{k+1} $ too.

### Computing $ \delta w^{k+1, 1}_1 $ 

$$ 
\delta w^{k+1, 1}_1 = \frac{\partial Loss}{\partial W^{k+1, 1}_1}(o^k_1)
$$

The interesting variable is $ W^{k+1, 1}_1 $. In the different diagrams it corresponds to $ w^{k+1, 1}_1 $, 
its value during the current **backward pass**. There is just one output of $ L^{k+1} $ that uses $ W^{k+1, 1}_1 $: 
$ L^{k+1, 1} $.
Indeed we can look at the diagram [here](#linear-structure1) to see that 
$ o^{k+1}_1 $ is linked to $ w^{k+1, 1}_1 $.

We are now able to build the **paths** of impacts from $ W^{k+1, 1}_1 $ to the $ Loss $ function. 
This is in fact what we already saw in [this diagram](#linear-structure3).

- $ W^{k+1, 1}_1 $ impacts $ L^{k+1, 1} $ which impacts the $ Loss $ function  

We have only 1 impact, using the **chain rule**, we obtain: 

$$ 
\delta w^{k+1, 1}_1 = \delta^{k+2, 1} * \frac{\partial L^{k+1, 1}}{W^{k+1, 1}_1}(o^k_1)
$$

We just have to compute: 

$$ 
\begin{align}
\frac{\partial L^{k+1, 1}}{\partial W^{k+1, 1}_1} &= 
\frac{\partial (W^{k+1, 1}_1 . X^{k+1, 1} + W^{k+1, 1}_2 . X^{k+1, 2} + W^{k+1, 1}_3 . X^{k+1, 3} + B^{k+1})}{\partial W^{k+1, 1}_1} \\
                                                 &= X^{k+1, 1}
\end{align}
$$

Then we evaluate this function on the values that have produced the final $ loss $:

$$ 
\frac{\partial L^{k+1, 1}}{W^{k+1, 1}_1}(o^k_1) = o^k_1
$$

We finally use this result in the first formula:

$$ 
\boxed{\delta w^{k+1, 1}_1 = \delta^{k+2, 1} * o^k_1}
$$

### Computing $ \delta w^{k+1, 1}_2 $ and $ \delta w^{k+1, 1}_3 $

Same as in the previous paragraph. 
The results are: 

$$ 
\boxed{\delta w^{k+1, 1}_2 = \delta^{k+2, 1} * o^k_2}
$$

$$ 
\boxed{\delta w^{k+1, 1}_3 = \delta^{k+2, 1} * o^k_3}
$$

### Computing $ \delta w^{k+1, 2}_1 $ 

$$ 
\delta w^{k+1, 2}_1 = \frac{\partial Loss}{\partial W^{k+1, 2}_1}(o^k_1)
$$

The interesting variable is $ W^{k+1, 2}_1 $. In the different diagrams it corresponds to $ w^{k+1, 2}_1 $, 
its value during the current **backward pass**. There is just one output of $ L^{k+1} $ that uses $ W^{k+1, 2}_1 $: 
$ L^{k+1, 2} $.
Indeed we can look at the diagram [here](#linear-structure2) to see that 
$ o^{k+1}_2 $ is linked to $ w^{k+1, 2}_1 $.

We are now able to build the **paths** of impacts from $ W^{k+1, 2}_1 $ to the $ Loss $ function. 
This is in fact what we already saw in [this diagram](#linear-structure4).

- $ W^{k+1, 2}_1 $ impacts $ L^{k+1, 2} $ which impacts the $ Loss $ function  

We have only 1 impact, using the **chain rule**, we obtain: 

$$ 
\delta w^{k+1, 2}_1 = \delta^{k+2, 2} * \frac{\partial L^{k+1, 2}}{W^{k+1, 2}_1}(o^k_1)
$$

We just have to compute: 

$$ 
\begin{align}
\frac{\partial L^{k+1, 2}}{\partial W^{k+1, 2}_1} &= 
\frac{\partial (W^{k+1, 2}_1 . X^{k+1, 1} + W^{k+1, 2}_2 . X^{k+1, 2} + W^{k+1, 2}_3 . X^{k+1, 3} + B^{k+1})}{\partial W^{k+1, 2}_1} \\
                                                  &= X^{k+1, 1}
\end{align}
$$

Then we evaluate this function on the values that have produced the final $ loss $:

$$ 
\frac{\partial L^{k+1, 2}}{W^{k+1, 2}_1}(o^k_1) = o^k_1
$$

We finally use this result in the first formula:

$$ 
\boxed{\delta w^{k+1, 2}_1 = \delta^{k+2, 2} * o^k_1}
$$

### Computing $ \delta w^{k+1, 2}_2 $ and $ \delta w^{k+1, 2}_3 $

Same as in the previous paragraph. 
The results are: 

$$ 
\boxed{\delta w^{k+1, 2}_2 = \delta^{k+2, 2} * o^k_2}
$$

$$ 
\boxed{\delta w^{k+1, 2}_3 = \delta^{k+2, 2} * o^k_3}
$$

### Computing $ \delta b^{k+1} $ 

$$ 
\delta b^{k+1} = \frac{\partial Loss}{\partial B^{k+1, 1}}(o^k)
$$

The interesting variable is $ B^{k+1} $. Every output of $ L^{k+1} $ uses $ B^{k+1} $. 
This is clear when we take a look at the first two equations of the [forward pass](#forward-pass) paragraph. 

We are now able to build the **paths** of impacts from $ B^{k+1} $ to the $ Loss $ function. 

- $ B^{k+1} $ impacts $ L^{k+1, 1} $ which impacts the $ Loss $ function  
- $ B^{k+1} $ impacts $ L^{k+1, 2} $ which impacts the $ Loss $ function  

We add these 2 impacts, using the **chain rule**, to obtain: 

$$ 
\delta b^{k+1} = \delta^{k+2, 1} * \frac{\partial L^{k+1, 1}}{B^{k+1}}(o^k) + 
\delta^{k+2, 2} * \frac{\partial L^{k+1, 2}}{B^{k+1}}(o^k)
$$

We just have to compute: 

$$ 
\begin{align}
\frac{\partial L^{k+1, 1}}{\partial B^{k+1}} &= 
\frac{\partial (W^{k+1, 1}_1 . X^{k+1, 1} + W^{k+1, 1}_2 . X^{k+1, 2} + W^{k+1, 1}_3 . X^{k+1, 3} + B^{k+1})}{\partial B^{k+1}} \\
                                             &= 1
\end{align}
$$

Then we evaluate this function on the values that have produced the final $ loss $:

$$ 
\frac{\partial L^{k+1, 1}}{B^{k+1}}(o^k) = 1
$$

We do the same to obtain: 

$$ 
\frac{\partial L^{k+1, 2}}{B^{k+1}}(o^k) = 1
$$

We finally assemble these two results:

$$ 
\boxed{\delta b^{k+1} = \delta^{k+2, 1} + \delta^{k+2, 2}}
$$

## Example

We have already used a $ Linear $ $ layer $ in the "Example" of the previous articles. Let us have a look 
at the $ model $ we used in the [weights article]({% post_url 2021-08-19-weights %}): 

$$
\begin{align}
    L1(X^1)  &= X^1 & \text{ with } X^1 = (X^1_1, X^1_2, X^1_3) \\
    L2(X^2, W^2) &= W^2 . X^2          & \text{ with } X^2 = (X^2_1, X^2_2, X^2_3) \\
                 &                     & \text{ and } W^2 = (W^2_1, W^2_2, W^2_3) \\
                 &= W^2_1 . X^2_1 + W^2_2 . X^2_2 + W^2_3 . X^2_3 \\
    L3(X^3)  &= X^3 \text{ if } X^3 \geq 0 \text{ else } 0 \\ \\
    model(X) &= L3(L2(L1(X))) & \text{ with } X = (X_1, X_2, X_3) \\ 
    Loss(X^4, Y^{truth})  &= \frac{1}{2} (X^4 - Y^{truth})^2 
\end{align}
$$

$ L2 $ is indeed a $ Linear $ $ layer $. 
It has two particularities compared to the **neural structure** we have seen [here](#linear-structure1). 

- It does not use any $ B^2 $ **bias**
- It has only 1 output **neuron** 

Note that the 1 output **neuron** is something that is necessary for the very structure of the problem we are 
trying to solve in the "Example" (see the [first article]({% post_url 2021-08-05-general-concepts %})): 
from **data input** that are 3 dimensional 
((broccoli, Tagada strawberries, workout hours) => 3 **neurons**) we want to predict **data output** that are 
1 dimensional ((good shape or not) => 1 **neuron**). 

Let us recap the $ L2 $ **neural structure**: 

![Linear](/_assets/images/layers/Linear11.png)

### <span style="text-decoration:underline"> Forward pass for L2 </span>

By definition, we just have to use the $ L2 $ explicit formula: 

$$ 
\begin{align}
    L2(X^2, W^2) &= W^2 . X^2          & \text{ with } X^2 = (X^2_1, X^2_2, X^2_3) \\
                 &                     & \text{ and } W^2 = (W^2_1, W^2_2, W^2_3) \\
                 &= W^2_1 . X^2_1 + W^2_2 . X^2_2 + W^2_3 . X^2_3 \\
\end{align}
$$ 

The **neural structure** we have seen in the [previous paragraph](#example) is another way to formalize the 
transformation of the $ L2 $ input values: $ o^1_1 $, $ o^1_2 $ and $ o^1_3 $ into the $ L2 $ output values: 
$ o^2 $. Still, it is more useful as a tool to find the impacts of the input **neurons** in order to compute 
the different elements of the  **backward pass**.

### <span style="text-decoration:underline"> Backward pass for L2 </span>

We have already computed the different elements. 

In the [backward pass for the learning flow](#backward-pass-for-the-learning-flow), we found:

$$
\delta^{k+1, 1} = \delta^{k+2, 1} * w^{k+1, 1}_1 + \delta^{k+2, 2} * w^{k+1, 2}_1
$$

$$ 
\delta^{k+1, 2} = \delta^{k+2, 1} * w^{k+1, 1}_2 + \delta^{k+2, 2} * w^{k+1, 2}_2
$$

$$ 
\delta^{k+1, 3} = \delta^{k+2, 1} * w^{k+1, 1}_3 + \delta^{k+2, 2} * w^{k+1, 2}_3
$$

We adjust these formula to the current **neural structure**: 

$$
\boxed{\delta^{2, 1} = \delta^{3} * w^{2}_1}
$$ 

$$ 
\boxed{\delta^{2, 2} = \delta^{3} * w^{2}_2}
$$

and

$$ 
\boxed{\delta^{2, 3} = \delta^{3} * w^{2}_3}
$$

We summarize them as: 

$$ 
\delta^{2} = \delta^{3} * w^2 
$$

which is what we already computed in the 
[backward pass article]({% post_url 2021-08-13-backward-pass %}).

<br>

In the [backward pass for the weights](#backward-pass-for-the-weights), we found:

$$ 
\delta w^{k+1, 1}_1 = \delta^{k+2, 1} * o^k_1
$$

$$ 
\delta w^{k+1, 1}_2 = \delta^{k+2, 1} * o^k_2
$$

$$ 
\delta w^{k+1, 1}_3 = \delta^{k+2, 1} * o^k_3
$$

In fact, we computed 6 formula for the **weights** and 1 formula for the **bias**. 
Considering the current **neural structure** it is clear we do not need the 3 last formula for the **weights** 
nor the formula for the **bias**. We just have to adapt the first 3 formula: 

$$ 
\boxed{\delta w^{2}_1 = \delta^{3} * o^1_1}
$$

$$ 
\boxed{\delta w^{2}_2 = \delta^{3} * o^1_2}
$$

$$ 
\boxed{\delta w^{2}_3 = \delta^{3} * o^1_3}
$$

which we summarize as:

$$ 
\delta w^{2} = \delta^{3} * o^1
$$

Once more, we recognize the formula we found in the [weights article]({% post_url 2021-08-19-weights %}).

## Conclusion

We have explored the $ Linear $ $ layer $ and found out we already used this $ layer $ in the 
"Example" of the previous chapters.

We saw how the **neural structure** helps finding the impacts of the different **neurons** on the $ Loss $ function.
This is the core of the **backward pass** and more generally the core of the **learning process**.

In the next article we will continue exploring the other $ layers $ of our "Example".
